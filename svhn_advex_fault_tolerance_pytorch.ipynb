{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduces all experiments in \"Adversarial Examples as an Input-Fault Tolerance Problem\"\n",
    "\n",
    "__Note about version control__:\n",
    "Select \"Restart & Clear Output\" from the \"Kernel\" tab before commiting changes so that diffs are interpretable. Otherwise spurious changes like the vector graphics and cell output will be tracked, and are likely to change between every commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform as t\n",
    "\n",
    "from ft_utils import arr21hot, mi, snr, build_targeted_dataset, evaluate_acc, evaluate_model\n",
    "from ft_plot import (fault_tolerance_plot, fault_tolerance_unique_obj, grid_visual,\n",
    "                     fault_tolerance_plot_rot_from_list_30, fault_tolerance_plot_rot_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = '0'  # None or physical id as a string\n",
    "SEED = 1\n",
    "\n",
    "if GPU is not None:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the repo light, yet self contained, we've uploaded 10K samples from the SVHN test set. We will later\n",
    "draw 1K samples from this set, but of course you are free to use all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVHN-specific dimensions\n",
    "img_rows = 32\n",
    "img_cols = 32\n",
    "channels = 1\n",
    "num_classes = 10\n",
    "\n",
    "##### ----- Data: Loading and Preprocessing ----- #####\n",
    "DATASET = 'dataset/svhn_test_set_10k.npz'\n",
    "\n",
    "dataset = np.load(DATASET)\n",
    "test_x = dataset['a']\n",
    "test_y = dataset['b']\n",
    "\n",
    "test_y_1hot = arr21hot(test_y, num_classes)\n",
    "test_x = test_x.reshape(-1, channels, img_rows, img_cols)\n",
    "test_x = test_x.astype('float32')\n",
    "oshape_test = test_x.shape\n",
    "\n",
    "# rescale to [0, 1]\n",
    "test_x /= 255.\n",
    "\n",
    "# subtract per-image mean\n",
    "test_x = test_x.reshape(-1, np.prod(test_x.shape[1:]))\n",
    "per_example_mean = np.mean(test_x, axis=1, keepdims=True)\n",
    "test_x -= per_example_mean\n",
    "\n",
    "# feature standardization\n",
    "feature_std = np.std(test_x, axis=0, ddof=0)\n",
    "test_x /= feature_std\n",
    "\n",
    "# reshape back to input_shape\n",
    "test_x = test_x.reshape(oshape_test)\n",
    "\n",
    "N = 1000  # number of samples from test set\n",
    "# np.random.seed(1234) # for testing different batches\n",
    "indx_arr = np.random.choice(len(test_y), N, replace=False)\n",
    "batch = torch.FloatTensor(test_x[indx_arr]).to(DEVICE)\n",
    "labels = torch.LongTensor(test_y[indx_arr]).to(DEVICE)\n",
    "labels_1hot = torch.LongTensor(\n",
    "    arr21hot(test_y[indx_arr], num_classes)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----- Hyper-params ----- #\n",
    "\n",
    "from cnn_model_pytorch import CNN, custom_init_params\n",
    "\n",
    "# Some of these (e.g. L2_WD, MB_SIZE, LR, and EPS)\n",
    "# don't matter as we are loading a pre-trained checkpoint,\n",
    "# but you can see the settings that were used for training.\n",
    "\n",
    "NUM_FILTERS = 32    # number of filters in the first conv layer\n",
    "BATCH_NORM = None  # arg of \"store true\"-type\n",
    "MB_SIZE = 128       # mini-batch size\n",
    "L2_WD = 1e-2      # L2 weight decay constant\n",
    "LR = 1e-2      # learning rate\n",
    "EPS = 50        # number of training epochs\n",
    "\n",
    "\n",
    "##### ----- Define the model ----- #####\n",
    "\n",
    "model = CNN(num_classes, NUM_FILTERS, channels, BATCH_NORM).to(DEVICE)\n",
    "model.apply(custom_init_params)\n",
    "\n",
    "loss_fnct = nn.CrossEntropyLoss()  # reduction='none'\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=L2_WD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load checkpoint for models trained with inverse-frequency\n",
    "class weights, zero-mean (per-image mean subtracted) grayscale data \n",
    "with 604k training examples total.\n",
    "'''\n",
    "#MODEL_FILE = 'ckpt/final_model_eps_50_seed_1_wd_0e+00.ckpt'\n",
    "\n",
    "'''\n",
    "Note that the ATTACK_ITERATIONS are calibrated to minimally sufficient defaults to \n",
    "see degradation in a reasonably short time assuming the checkpoint with wd 1e-2 is used. \n",
    "The model without wd is much more sensitive and thus doesn't require much change to the \n",
    "input to make wrong predictions with high confidence. You will have to tune the \n",
    "iterations yourself for wd_0e+00; we suggest increasing by at least one order of \n",
    "magnitude as a starting point.\n",
    "'''\n",
    "MODEL_FILE = 'ckpt/final_model_eps_50_seed_1_wd_1e-02.ckpt'\n",
    "\n",
    "if os.path.exists(MODEL_FILE):\n",
    "    model.load_state_dict(torch.load(\n",
    "        MODEL_FILE, map_location=lambda storage, loc: storage))\n",
    "    model = model.to(DEVICE)\n",
    "else:\n",
    "    print('Model not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Evaluate model on clean data ----- #####\n",
    "model.eval()\n",
    "test_accuracy = evaluate_acc(model, batch, labels)\n",
    "# on the 1k random batch, accuracy should be 0.9400 for model without wd, or 0.9190 with wd\n",
    "print('Prediction accuracy on test data: %.4f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PLOTS = False\n",
    "basefilename = MODEL_FILE.split('/')[-1].split('.')[0]\n",
    "print(basefilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning names to index values\n",
    "SNR = 0\n",
    "ITY = 1\n",
    "ACC = 2\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "SAVE_DIR = \"npdata/pytorch/mi/\"  # where to save all outputs for plots\n",
    "print(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIM $L_2$ Attack with Misclassification Objective\n",
    "\n",
    "This could take a long time to complete the whole range of SNR down to zero. \n",
    "In general, the model without weight decay takes more iterations due to \n",
    "vanishing gradients. Defaults are set so as to provide a suitable reward in a \n",
    "reasonable amount of time (< 10s on Titan Xp GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_L2 = 500\n",
    "\n",
    "adv_img_l2 = batch.clone()\n",
    "ft_l2 = np.zeros((ATTACK_ITER_L2, 3))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(ATTACK_ITER_L2):\n",
    "    '''\n",
    "    Note that we increase epsilon here as a kind of momentum since we have a log scale x-axis, \n",
    "    can run fixed epsilon instead to see that results are similar, but this takes longer.\n",
    "    '''\n",
    "    eps_ = np.clip(0.01 + (float(i) / 10**4), 0., 0.1)\n",
    "\n",
    "    x_ = Variable(adv_img_l2, requires_grad=True)\n",
    "    red_ind = list(np.arange(1, len(x_.shape)))\n",
    "\n",
    "    loss = loss_fnct(model(x_), labels)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    square = torch.max(torch.FloatTensor([1e-12]).to(DEVICE),  # to prevent div by zero\n",
    "                       torch.sum(loss_grad**2, dim=red_ind, keepdim=True))\n",
    "    normalized_loss_grad = loss_grad / torch.sqrt(square)\n",
    "    adv_img_l2 = x_.detach() + (eps_ * normalized_loss_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds_l2 = softmax(model(adv_img_l2))\n",
    "\n",
    "    ft_l2[i, ACC] = evaluate_acc(model, adv_img_l2, labels)\n",
    "    ft_l2[i, SNR] = snr(batch, adv_img_l2 - batch)\n",
    "    ft_l2[i, ITY] = mi(torch.argmax(adv_preds_l2, 1), labels)\n",
    "\n",
    "print(time.time() - start_time)\n",
    "# may need to bump up max_snr, e.g. to 95 to see results for model w/out wd\n",
    "fault_tolerance_plot(ft_l2, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_bim_l2_mcls.npy', ft_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive White Gaussian Noise \n",
    "This cell should also take about 5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_GAUSS = 100\n",
    "sigmas = np.linspace(1e-6, 10, ATTACK_ITER_GAUSS)\n",
    "ft_noise = np.zeros((ATTACK_ITER_GAUSS, 3))\n",
    "start_time = time.time()\n",
    "for i, s in enumerate(sigmas):\n",
    "    noise_gauss = np.random.normal(loc=0.0, scale=float(s),\n",
    "                                   size=(N, channels, img_rows, img_cols))\n",
    "    noise_gauss = torch.FloatTensor(noise_gauss).to(DEVICE)\n",
    "\n",
    "    noise_preds_np = softmax(model(batch + noise_gauss))\n",
    "\n",
    "    ft_noise[i, ACC] = evaluate_acc(model, batch + noise_gauss, labels)\n",
    "    ft_noise[i, SNR] = snr(batch, noise_gauss)\n",
    "    ft_noise[i, ITY] = mi(torch.argmax(noise_preds_np, 1), labels)\n",
    "print(time.time() - start_time)\n",
    "fault_tolerance_plot(ft_noise, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_noise.npy', ft_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIM $L_2$ Attack with Targeted Objective\n",
    "\n",
    "There are two different targeted datasets that are interesting to visualize in terms of information $I(T; Y)$\n",
    "1. Each class label is replaced with a specific target class (`labels_t`)\n",
    "2. Each input is repeated `num_classes-1` times, once for each `class != original label`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: 'one tgt.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct targeted labels for case 1:\n",
    "labels_t_1hot = np.roll(labels_1hot, 1, axis=1)  # target labels, 1-hot\n",
    "labels_t = torch.LongTensor(\n",
    "    np.argmax(labels_t_1hot, axis=1)).to(DEVICE)  # target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_L2 = 500\n",
    "\n",
    "ft_l2_t = np.zeros((ATTACK_ITER_L2, 3))\n",
    "adv_img_l2 = batch.clone()\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(ATTACK_ITER_L2):\n",
    "\n",
    "    eps_ = np.clip(0.01 + (float(i) / 10**4), 0., 0.1)\n",
    "\n",
    "    x_ = Variable(adv_img_l2, requires_grad=True)\n",
    "    red_ind = list(np.arange(1, len(x_.shape)))\n",
    "\n",
    "    loss = loss_fnct(model(x_), labels_t)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    square = torch.max(torch.FloatTensor([1e-12]).to(DEVICE),\n",
    "                       torch.sum(loss_grad**2, dim=red_ind, keepdim=True))\n",
    "    normalized_loss_grad = loss_grad / torch.sqrt(square)\n",
    "    adv_img_l2 = x_.detach() - (eps_ * normalized_loss_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds_l2 = softmax(model(adv_img_l2))\n",
    "\n",
    "    ft_l2_t[i, ACC] = evaluate_acc(model, adv_img_l2, labels)\n",
    "    ft_l2_t[i, SNR] = snr(batch, adv_img_l2 - batch)\n",
    "    ft_l2_t[i, ITY] = mi(torch.argmax(adv_preds_l2, 1), labels)\n",
    "\n",
    "print(time.time() - start_time)\n",
    "fault_tolerance_plot(ft_l2_t, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_l2_t[-1][0] = 0. # optionally extend to SNR 0 if accuracy/MI plateaus\n",
    "#np.save(SAVE_DIR + basefilename + '_ft_bim_l2_one_tgt.npy', ft_l2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: 'all tgt.'\n",
    "This cell takes longer because we repeat each of the `N` samples `num_classes - 1` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_target_classes = num_classes - 1\n",
    "orig_img = torch.FloatTensor(\n",
    "    np.repeat(test_x[indx_arr], num_target_classes, axis=0)).to(DEVICE)\n",
    "true_labels = torch.LongTensor(\n",
    "    np.repeat(test_y[indx_arr], num_target_classes, axis=0)).to(DEVICE)\n",
    "a = np.repeat([np.arange(num_classes)], len(test_y[indx_arr]), axis=0)\n",
    "target_labels = torch.LongTensor(\n",
    "    a[a != np.array(test_y[indx_arr])[:, None]]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_L2 = 200  # takes about 40 seconds\n",
    "\n",
    "ft_l2_t_all = np.zeros((ATTACK_ITER_L2, 3))\n",
    "adv_img_l2_t = orig_img.clone()\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(ATTACK_ITER_L2):\n",
    "\n",
    "    eps_ = np.clip(0.01 + (float(i) / 10**4), 0., 0.1)\n",
    "    x_ = Variable(adv_img_l2_t, requires_grad=True)\n",
    "    red_ind = list(np.arange(1, len(x_.shape)))\n",
    "\n",
    "    loss = loss_fnct(model(x_), target_labels)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    square = torch.max(torch.FloatTensor([1e-12]).to(DEVICE),\n",
    "                       torch.sum(loss_grad**2, dim=red_ind, keepdim=True))\n",
    "    normalized_loss_grad = loss_grad / torch.sqrt(square)\n",
    "    adv_img_l2_t = x_.detach() - (eps_ * normalized_loss_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds_l2_t = softmax(model(adv_img_l2_t))\n",
    "\n",
    "    ft_l2_t_all[i, ACC] = evaluate_acc(model, adv_img_l2_t, true_labels)\n",
    "    ft_l2_t_all[i, SNR] = snr(orig_img, adv_img_l2_t - orig_img)\n",
    "    ft_l2_t_all[i, ITY] = mi(torch.argmax(adv_preds_l2_t, 1), true_labels)\n",
    "print(time.time() - start_time)\n",
    "# ft_l2_t_all[-1][0] = 0. # optionally extend to SNR 0\n",
    "fault_tolerance_plot(ft_l2_t_all, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_bim_l2_all_tgt.npy', ft_l2_t_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the plot style from the paper\n",
    "fault_tolerance_unique_obj([ft_noise, ft_l2, ft_l2_t, ft_l2_t_all],\n",
    "                           legend=False, labels=['awgn', 'mis-cls.', 'one tgt.', 'all tgt.'],\n",
    "                           save=False, min_snr=0, max_snr=65, modelname=basefilename + '_bim_l2_n%d_' % N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIM $L_\\infty$ Attack with Misclassification Objective\n",
    "\n",
    "The sign method is less efficient wrt a multi-layer network, so we don't require as many iterations to cause \n",
    "lots of degradation in signal quality and span most of the SNR curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_INF = 500\n",
    "\n",
    "ft_inf = np.zeros((ATTACK_ITER_INF, 3))\n",
    "adv_img_inf = batch.clone()\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(ATTACK_ITER_INF):\n",
    "\n",
    "    eps_ = np.clip(1e-4 + (float(i) / 10**4), 0., 0.1)\n",
    "    x_ = Variable(adv_img_inf, requires_grad=True)\n",
    "    red_ind = list(np.arange(1, len(x_.shape)))\n",
    "\n",
    "    loss = loss_fnct(model(x_), labels)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    signed_grad = torch.sign(loss_grad)\n",
    "    adv_img_inf = x_.detach() + (eps_ * signed_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds_inf = softmax(model(adv_img_inf))\n",
    "\n",
    "    ft_inf[i, ACC] = evaluate_acc(model, adv_img_inf, labels)\n",
    "    ft_inf[i, SNR] = snr(batch, adv_img_inf - batch)\n",
    "    ft_inf[i, ITY] = mi(torch.argmax(adv_preds_inf, 1), labels)\n",
    "print(time.time() - start_time)\n",
    "fault_tolerance_plot(ft_inf, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_bim_inf_mcls.npy', ft_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIM $L_\\infty$ Attack with Targeted Objective\n",
    "\n",
    "There are two different targeted datasets that are interesting to visualize in terms of information $I(T; Y)$\n",
    "1. Each class label is replaced with a specific target class (`labels_t`)\n",
    "2. Each input is repeated `num_classes - 1` times, once for each `class != original label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_INF = 500\n",
    "\n",
    "ft_inf_t = np.zeros((ATTACK_ITER_INF, 3))\n",
    "adv_img_inf_one_tgt = batch.clone()\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(ATTACK_ITER_INF):\n",
    "\n",
    "    eps_ = np.clip(1e-4 + (float(i) / 10**4), 0., 0.1)\n",
    "    x_ = Variable(adv_img_inf_one_tgt, requires_grad=True)\n",
    "\n",
    "    loss = loss_fnct(model(x_), labels_t)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    signed_grad = torch.sign(loss_grad)\n",
    "    adv_img_inf_one_tgt = x_.detach() - (eps_ * signed_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds_inf_one_tgt = softmax(model(adv_img_inf_one_tgt))\n",
    "\n",
    "    ft_inf_t[i, ACC] = evaluate_acc(model, adv_img_inf_one_tgt, labels)\n",
    "    ft_inf_t[i, SNR] = snr(batch, adv_img_inf_one_tgt - batch)\n",
    "    ft_inf_t[i, ITY] = mi(torch.argmax(adv_preds_inf_one_tgt, 1), labels)\n",
    "print(time.time() - start_time)\n",
    "# ft_inf_t[-1][0] = 0. # optionally extend to SNR 0\n",
    "fault_tolerance_plot(ft_inf_t, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_bim_inf_one_tgt.npy', ft_inf_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: 'all tgt.'\n",
    "This cell takes longer because we repeat each of the `N` samples `num_classes - 1` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_INF = 100  # takes about 40 seconds\n",
    "\n",
    "ft_inf_t_all = np.zeros((ATTACK_ITER_INF, 3))\n",
    "adv_img_inf_t = orig_img.clone()\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(ATTACK_ITER_INF):\n",
    "\n",
    "    eps_ = np.clip(1e-4 + (float(i) / 10**4), 0., 0.1)\n",
    "    x_ = Variable(adv_img_inf_t, requires_grad=True)\n",
    "    loss = loss_fnct(model(x_), target_labels)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    signed_grad = torch.sign(loss_grad)\n",
    "    adv_img_inf_t = x_.detach() - (eps_ * signed_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds_inf_t = softmax(model(adv_img_inf_t))\n",
    "\n",
    "    ft_inf_t_all[i, ACC] = evaluate_acc(model, adv_img_inf_t, true_labels)\n",
    "    ft_inf_t_all[i, SNR] = snr(orig_img, adv_img_inf_t - orig_img)\n",
    "    ft_inf_t_all[i, ITY] = mi(torch.argmax(adv_preds_inf_t, 1), true_labels)\n",
    "print(time.time() - start_time)\n",
    "# ft_inf_t_all[-1][0] = 0. # optionally extend to SNR 0\n",
    "fault_tolerance_plot(ft_inf_t_all, max_snr=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_bim_inf_all_tgt.npy', ft_inf_t_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_tolerance_unique_obj([ft_noise, ft_inf, ft_inf_t, ft_inf_t_all],\n",
    "                           legend=False, labels=['awgn', 'mis-cls.', 'one tgt.', 'all tgt.'],\n",
    "                           save=False, max_snr=65, modelname=basefilename + '_inf_n%d_' % N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare BIM-$L_2$ and BIM-$L_\\infty$ Misclassification Attacks\n",
    "The $L_\\infty$ variant should be in between $L_2$ and awgn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_tolerance_unique_obj([ft_noise, ft_l2, ft_inf], \n",
    "                            legend=True, save=False, \n",
    "                            labels=['awgn', r'$L_2$', r'$L_\\infty$'], \n",
    "                            max_snr=65,\n",
    "                            modelname=basefilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted Attack Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = 10\n",
    "# optionally shuffle the data to generate new images\n",
    "'''\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(test_y)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(test_x)\n",
    "'''\n",
    "idxs_arr = np.zeros(NT, dtype='int')\n",
    "instances_per_class = NT // num_classes\n",
    "j = 0\n",
    "for i in range(num_classes):\n",
    "    idxs_arr[j:j +\n",
    "             instances_per_class] = [np.where(test_y == i)][0][0][:instances_per_class]\n",
    "    j += instances_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_images, unif_labels, adv_ys = build_targeted_dataset(\n",
    "    test_x, test_y, idxs_arr, num_classes, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ITER_GRID = 75  # takes about 20 seconds\n",
    "ft_grid = np.zeros((ATTACK_ITER_GRID, 3))\n",
    "adv_img = unif_images.clone()\n",
    "start_time = time.time()\n",
    "eps_ = 0.1\n",
    "for i in range(ATTACK_ITER_GRID):\n",
    "\n",
    "    #eps_ = np.clip(0.1 + (float(i) / 10**4), 0., 0.1)\n",
    "    x_ = Variable(adv_img, requires_grad=True)\n",
    "    red_ind = list(np.arange(1, len(x_.shape)))\n",
    "    loss = loss_fnct(model(x_), adv_ys)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    square = torch.max(torch.FloatTensor([1e-12]).to(DEVICE),\n",
    "                       torch.sum(loss_grad**2, dim=red_ind, keepdim=True))\n",
    "    normalized_loss_grad = loss_grad / torch.sqrt(square)\n",
    "    adv_img = x_.detach() - (eps_ * normalized_loss_grad).to(DEVICE)\n",
    "\n",
    "    adv_img_preds = softmax(model(adv_img))\n",
    "    ft_grid[i, ACC] = evaluate_acc(model, adv_img, unif_labels)\n",
    "    ft_grid[i, SNR] = snr(unif_images, adv_img - unif_images)\n",
    "    ft_grid[i, ITY] = mi(torch.argmax(adv_img_preds, 1), unif_labels)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_shape = (num_classes, num_classes, img_rows, img_cols)\n",
    "grid_viz_data = np.zeros(grid_shape, dtype='f')\n",
    "n = num_classes - 1\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        # the original image go along the diagonal\n",
    "        if i == j:\n",
    "            idx = j * n\n",
    "            grid_viz_data[i, j] = np.squeeze(unif_images[j * n])\n",
    "        else:\n",
    "            if i > j:\n",
    "                idx = j * n + i - 1\n",
    "            else:\n",
    "                idx = j * n + i\n",
    "            grid_viz_data[i, j] = np.squeeze(adv_img[idx])\n",
    "fig = grid_visual(grid_viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sorted = np.sort(adv_img_preds.detach().cpu().numpy())\n",
    "margin = np.mean(preds_sorted[:, 9] - preds_sorted[:, 8])\n",
    "#fig.savefig(basefilename + 'tgt_attk_grid-%.1e.png' % margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Deformations (ADef)\n",
    "`candidates` refers to the candidate target labels as in the original DeepFool algorithm. Feel free to experiment! \n",
    "\n",
    "- `candidates = range(10)` targets all possible incorrect labels, i.e., a misclassification attack.\n",
    "- `candidates = 9` will target the least likely class. This is the most difficult targeted attack, thus it should require the most distortion of the input, which means further to the right on the I(T;Y)-vs-max_norm plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = range(10) # the indices of labels to target in the ordering of descending confidence\n",
    "#candidates = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deformation import ADef\n",
    "import ADef_on_SVHN_funcs as funcs\n",
    "\n",
    "# optionally save ADef output, i.e., deformed images, vector fields etc.\n",
    "SAVE_DATA = False\n",
    "\n",
    "# -------------  specify ADef config: --------------\n",
    "max_iter = 100        # max number of iterations for deformation\n",
    "sigma = 0.5           # deformation smoothing parameter (Gaussian kernel width)\n",
    "overshoot = 1.2       # how much to overshoot each prediction to overcome zero gradients\n",
    "strong_targets = True  # False to stop as soon as model misclassifies input, True to stop only once a candidate label is achieved\n",
    "do_plot = False\n",
    "verbose = False\n",
    "\n",
    "iparam_type = 'norm'\n",
    "# for quickly generating adef images\n",
    "iparam_arr = np.arange(3.5, 4, step=.5)\n",
    "# for full fault tolerance curve\n",
    "#iparam_arr = np.hstack((np.arange(0.1, 3, step=.2),\n",
    "#                        np.arange(3, 4, step=.5)))\n",
    "# --------------------------------------------------\n",
    "#margins = np.zeros(( len(iparam_arr), batch_size ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create deformed images for the model, evaluate acc and margins, compute MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_adef = batch.clone()\n",
    "\n",
    "# First get clean data point so we have a suitable y-axis intercept.\n",
    "acc = [evaluate_acc(model, orig_adef, labels)]\n",
    "\n",
    "predicted = softmax(model(orig_adef))\n",
    "Iy = [mi(torch.argmax(predicted, 1), labels)]\n",
    "acc = [evaluate_acc(model, orig_adef, labels)]\n",
    "print(Iy)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# deform images using ADef\n",
    "for iparam_indx, iparam in enumerate(iparam_arr):\n",
    "    max_norm = iparam\n",
    "    print('Doing %s %.2f' % (iparam_type, iparam))\n",
    "\n",
    "    def_batch, def_data = ADef(orig_adef, model, ind_candidates=candidates,\n",
    "                               max_norm=max_norm, max_iter=max_iter,\n",
    "                               smooth=sigma, overshoot=overshoot,\n",
    "                               targeting=strong_targets, verbose=verbose)\n",
    "\n",
    "    # saves adversarial candidate images\n",
    "    if SAVE_DATA:\n",
    "        np.savez_compressed(\n",
    "            save_dir + 'adef_images_norm_%.1f.npz' % iparam, a=def_batch)\n",
    "        np.savez_compressed(\n",
    "            save_dir + 'adef_data_norm_%.1f.npz' % iparam, a=def_data)\n",
    "\n",
    "    # prediction accuracy on deformed images\n",
    "    acc.append(evaluate_acc(model, def_batch, labels))\n",
    "    predicted = softmax(model(def_batch))\n",
    "\n",
    "    # compute MI between T=predicted and Y=label\n",
    "    Iy.append(mi(torch.argmax(predicted, 1), labels))\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "# avg margins\n",
    "#M = np.hstack((None, np.mean(margins, axis=1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ADef examples and prediction margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adef_images = def_batch.detach().cpu().numpy()\n",
    "orig_x = orig_adef.detach().cpu().numpy()\n",
    "adef_preds_np = predicted.detach().cpu().numpy()\n",
    "orig_preds = softmax(model(orig_adef))\n",
    "orig_preds_np = orig_preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np = labels.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEK = 40\n",
    "font_size = 16\n",
    "fig, axes = plt.subplots(3, 10, squeeze=True, figsize=(12, 4.5))\n",
    "for idx in range(SEEK, SEEK + num_classes):\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            axes[i, idx - SEEK].imshow(np.squeeze(orig_x[idx]))\n",
    "            pred_class = np.argmax(orig_preds_np[idx])\n",
    "            preds_sorted = np.sort(orig_preds_np[idx])\n",
    "            margin = preds_sorted[9] - preds_sorted[8]\n",
    "            font_color = 'k' if pred_class == labels_np[idx] else 'r'\n",
    "            axes[i, idx - SEEK].set_title('%d: %.f\\%%' % (pred_class, 100 * margin),\n",
    "                                          fontsize=font_size, color=font_color)\n",
    "        elif i == 1:\n",
    "            axes[i, idx - SEEK].imshow(np.squeeze(adef_images[idx]))\n",
    "            adef_pred_class = np.argmax(adef_preds_np[idx])\n",
    "            adef_preds_sorted = np.sort(adef_preds_np[idx])\n",
    "            adef_margin = adef_preds_sorted[9] - adef_preds_sorted[8]\n",
    "            font_color = 'g' if adef_pred_class == labels_np[idx] else 'k'\n",
    "            axes[i, idx - SEEK].set_title('%d: %.f\\%%' % (adef_pred_class, 100 * adef_margin),\n",
    "                                          fontsize=font_size, color=font_color)\n",
    "        else:\n",
    "            axes[i, idx -\n",
    "                 SEEK].imshow(np.squeeze(adef_images[idx] - orig_x[idx]))\n",
    "        axes[i, idx - SEEK].get_xaxis().set_visible(False)\n",
    "        axes[i, idx - SEEK].get_yaxis().set_visible(False)\n",
    "plt.tight_layout(pad=0.1)\n",
    "plt.subplots_adjust(top=0.92, bottom=0.01, left=0.01,\n",
    "                    right=0.99, hspace=.15, wspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.savefig(basefilename + '_adef_candidates%d_seek%d.png' % (candidates, SEEK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for axis 1 in data\n",
    "SNR = 0\n",
    "ITY = 1\n",
    "ACC = 2\n",
    "\n",
    "ssorfb = 'fullbatch'\n",
    "forplot = np.zeros((len(iparam_arr) + 1, 4))\n",
    "forplot[:, SNR] = np.hstack((0, iparam_arr))\n",
    "forplot[:, ITY] = Iy\n",
    "forplot[:, ACC] = acc\n",
    "\n",
    "attack_type = 'tgt' if candidates == 9 else 'mcls'\n",
    "file_save = SAVE_DIR + basefilename + \\\n",
    "    '_adef_data_to_plot_%s_%s' % (ssorfb, attack_type)\n",
    "#np.save(file_save, forplot)\n",
    "print('* * * saved data to %s * * *' % SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if candidates == 9:\n",
    "    label = r'one tgt.'\n",
    "else:\n",
    "    label = r'mis-cls.'\n",
    "fault_tolerance_unique_obj_ADef([forplot[:, :3]],\n",
    "                                legend=True,\n",
    "                                labels=[label],\n",
    "                                save=False,\n",
    "                                plot_name=basefilename + '_adef_%s.eps' % attack_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssorfb = 'fullbatch'\n",
    "SAVE_DIR = 'npdata/pytorch/ami/'\n",
    "tgt_data = np.load(SAVE_DIR + basefilename +\n",
    "                   '_adef_data_to_plot_%s_tgt.npy' % ssorfb)\n",
    "mcls_data = np.load(SAVE_DIR + basefilename +\n",
    "                    '_adef_data_to_plot_%s_mcls.npy' % ssorfb)\n",
    "\n",
    "fault_tolerance_unique_obj_ADef([mcls_data[:, :3], tgt_data[:, :3]],\n",
    "                                legend=True,\n",
    "                                labels=[r'mis-cls.', r'tgt. (ll)'],\n",
    "                                save=False,\n",
    "                                plot_name=basefilename + '_adef_cmp_goals.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot30 = np.linspace(0, np.pi / 6)\n",
    "ft_rot_30 = np.zeros((rot30.shape[0], 3))\n",
    "test_x_rot = np.zeros((N, 1, 32, 32))\n",
    "'''\n",
    "t.warp requires float images between -1, and 1. \n",
    "scale by SCALAR for warp, then scale back to natural \n",
    "zero-mean, unit-variance range to evaluate model\n",
    "'''\n",
    "SCALAR = 10.\n",
    "for i, r in enumerate(rot30):\n",
    "    for j in range(N):\n",
    "        tform = t.SimilarityTransform(scale=1, rotation=r, translation=(0, 0))\n",
    "        # had to squeeze and unsqueeze to deal with NCHW format, rotate won't work for shape (1, 32, 32)\n",
    "        test_x_rot[j, :] = np.expand_dims(\n",
    "            t.warp(np.squeeze(test_x[indx_arr][j]) / SCALAR, tform, mode='wrap'), axis=0)\n",
    "    test_x_rot_cuda = torch.FloatTensor(test_x_rot * SCALAR).to(DEVICE)\n",
    "    rot_preds_np = softmax(model(test_x_rot_cuda))\n",
    "    ft_rot_30[i, ACC] = evaluate_acc(model, test_x_rot_cuda, labels)\n",
    "    ft_rot_30[i, ITY] = mi(torch.argmax(rot_preds_np, 1), labels)\n",
    "\n",
    "fault_tolerance_plot_rot_from_list_30(\n",
    "    rot30, [ft_rot_30], legend=False, save=False, labels=[r'CNN-$\\lambda$-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_rot30_test.npy', ft_rot_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 180 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot180 = np.linspace(0, np.pi)\n",
    "ft_rot_180 = np.zeros((rot180.shape[0], 3))\n",
    "test_x_rot = np.zeros((N, 1, 32, 32))\n",
    "for i, r in enumerate(rot180):\n",
    "    for j in range(N):\n",
    "        tform = t.SimilarityTransform(scale=1, rotation=r, translation=(0, 0))\n",
    "        test_x_rot[j, :] = np.expand_dims(\n",
    "            t.warp(np.squeeze(test_x[indx_arr][j]) / SCALAR, tform, mode='wrap'), axis=0)\n",
    "    test_x_rot_cuda = torch.FloatTensor(test_x_rot * SCALAR).to(DEVICE)\n",
    "    rot_preds_np = softmax(model(test_x_rot_cuda))\n",
    "    ft_rot_180[i, ACC] = evaluate_acc(model, test_x_rot_cuda, labels)\n",
    "    ft_rot_180[i, ITY] = mi(torch.argmax(rot_preds_np, 1), labels)\n",
    "\n",
    "fault_tolerance_plot_rot_from_list(rot180, [\n",
    "                                   ft_rot_180], legend=False, save=False, max_rot=180, labels=[r'CNN-$\\lambda$-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(SAVE_DIR + basefilename + '_ft_rot180_test.npy', ft_rot_180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fooling Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a rectangle in axes coords\n",
    "left, width = .25, .5\n",
    "bottom, height = .25, .5\n",
    "right = left + width\n",
    "top = bottom + height\n",
    "\n",
    "EPS_PER_STEP = 0.1\n",
    "\n",
    "fool_labels = np.zeros((1,))\n",
    "\n",
    "MEAN = 0.\n",
    "STDDEV = 1e-1\n",
    "FOOL_ITER = 100\n",
    "ANNOTATE_MARGIN = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, squeeze=True, figsize=(12, 2))\n",
    "\n",
    "for j in range(num_classes):\n",
    "    adv_img = torch.FloatTensor(\n",
    "        np.random.normal(loc=MEAN, scale=STDDEV, size=(\n",
    "            1, channels, img_rows, img_cols))).to(DEVICE)\n",
    "    fool_labels[:] = j\n",
    "    for k in range(FOOL_ITER):\n",
    "        x_ = Variable(adv_img, requires_grad=True)\n",
    "        red_ind = list(np.arange(1, len(x_.shape)))\n",
    "\n",
    "        loss = loss_fnct(model(x_), torch.LongTensor(fool_labels).to(DEVICE))\n",
    "        loss.backward()\n",
    "        loss_grad = x_.grad.data.clone()\n",
    "        square = torch.max(torch.FloatTensor([1e-12]).to(DEVICE),  # to prevent div by zero\n",
    "                           torch.sum(loss_grad**2, dim=red_ind, keepdim=True))\n",
    "        normalized_loss_grad = loss_grad / torch.sqrt(square)\n",
    "        adv_img = x_.detach() - (EPS_PER_STEP * normalized_loss_grad).to(DEVICE)\n",
    "\n",
    "    adv_preds = softmax(model(adv_img))\n",
    "    axes[j].imshow(adv_img.reshape(img_rows, img_cols),\n",
    "                   cmap='gray', vmin=.381, vmax=.598)\n",
    "    axes[j].imshow(adv_img.reshape(img_rows, img_cols), cmap='gray')\n",
    "    axes[j].get_xaxis().set_visible(False)\n",
    "    axes[j].get_yaxis().set_visible(False)\n",
    "\n",
    "    if ANNOTATE_MARGIN:\n",
    "        preds_np = adv_preds.detach().cpu().numpy()\n",
    "        preds_sorted = np.sort(preds_np)\n",
    "        margin = preds_sorted[0, 9] - preds_sorted[0, 8]\n",
    "        pred_idx = np.argmax(preds_np[0, :])\n",
    "        ax = axes[j]\n",
    "        ax.text(0.5 * (left + right), -0.3, '%d: %.3f' % (pred_idx, np.round(margin, decimals=3)),\n",
    "                horizontalalignment='center', verticalalignment='bottom', rotation=0,\n",
    "                transform=ax.transAxes, size='larger')\n",
    "PLT_NAME = basefilename + \\\n",
    "    '_bim-ord2-%ditr_gauss-u%.e-std%.e.png' % (FOOL_ITER, MEAN, STDDEV)\n",
    "print(PLT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.savefig(PLT_NAME, bbox_inches='tight', format='png')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
